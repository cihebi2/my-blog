---
title: "GraphRAG å®Œæ•´å…¥é—¨æŒ‡å—ï¼ˆäºŒï¼‰ï¼šæ ¸å¿ƒæŠ€æœ¯æ·±åº¦è§£æ"
author: "Ciheb"
pubDatetime: 2025-07-04T10:24:22Z
description: "æ·±å…¥è§£æGraphRAGçš„æ ¸å¿ƒæŠ€æœ¯å®ç°ï¼ŒåŒ…æ‹¬å®ä½“å…³ç³»æŠ½å–ã€å›¾æ„å»ºç­–ç•¥ã€å¤šè·³æ¨ç†æœºåˆ¶å’Œæ··åˆæ£€ç´¢æŠ€æœ¯"
tags: ["GraphRAG", "å®ä½“å…³ç³»æŠ½å–", "å›¾æ„å»º", "å¤šè·³æ¨ç†", "æŠ€æœ¯å®ç°", "æ·±åº¦å­¦ä¹ "]
featured: true
draft: false
---

## å‰è¨€

åœ¨[ä¸Šä¸€ç¯‡æ–‡ç« ](./graphrag-complete-guide-part1)ä¸­ï¼Œæˆ‘ä»¬ä»‹ç»äº†GraphRAGçš„åŸºç¡€æ¦‚å¿µå’Œæ ¸å¿ƒä¼˜åŠ¿ã€‚æœ¬ç¯‡å°†æ·±å…¥æ¢è®¨GraphRAGçš„æ ¸å¿ƒæŠ€æœ¯å®ç°ï¼ŒåŒ…æ‹¬å®ä½“å…³ç³»æŠ½å–ã€å›¾æ„å»ºç­–ç•¥ã€å¤šè·³æ¨ç†æœºåˆ¶å’Œæ··åˆæ£€ç´¢æŠ€æœ¯ã€‚

> **ğŸ“‹ ç³»åˆ—å¯¼èˆª**ï¼š
> - [ç¬¬ä¸€ç¯‡ï¼šåŸºç¡€æ¦‚å¿µä¸æ ¸å¿ƒä¼˜åŠ¿](./graphrag-complete-guide-part1)
> - **ç¬¬äºŒç¯‡ï¼šæ ¸å¿ƒæŠ€æœ¯æ·±åº¦è§£æï¼ˆå½“å‰ï¼‰**
> - ç¬¬ä¸‰ç¯‡ï¼šå·¥å…·æ¡†æ¶æ·±åº¦åˆ†æï¼ˆå³å°†å‘å¸ƒï¼‰
> - ç¬¬å››ç¯‡ï¼šåº”ç”¨å®è·µä¸å­¦ä¹ è·¯å¾„ï¼ˆå³å°†å‘å¸ƒï¼‰

## GraphRAGæ ¸å¿ƒæŠ€æœ¯æ·±åº¦è§£æ

### å®ä½“è¯†åˆ«ä¸å…³ç³»æŠ½å–ï¼šæ„å»ºçŸ¥è¯†å›¾è°±çš„åŸºçŸ³

GraphRAGé‡‡ç”¨åŸºäºå¤§è¯­è¨€æ¨¡å‹çš„å®ä½“å…³ç³»æŠ½å–æ–¹æ³•ï¼Œç›¸æ¯”ä¼ ç»Ÿçš„åŸºäºè§„åˆ™æˆ–ç»Ÿè®¡çš„æ–¹æ³•å…·æœ‰æ˜¾è‘—ä¼˜åŠ¿ã€‚**LLMé©±åŠ¨çš„æŠ½å–è¿‡ç¨‹**èƒ½å¤Ÿç†è§£å¤æ‚çš„ä¸Šä¸‹æ–‡è¯­ä¹‰ï¼Œè¯†åˆ«éšå«çš„å…³ç³»æ¨¡å¼ã€‚

#### æŠ€æœ¯å®ç°æµç¨‹

**1. æ–‡æœ¬åˆ†å—å¤„ç†**

å°†æ–‡æ¡£åˆ†å‰²ä¸º200-600ä¸ªtokençš„æ–‡æœ¬å•å…ƒï¼Œä¿è¯LLMèƒ½å¤Ÿå……åˆ†ç†è§£ä¸Šä¸‹æ–‡ã€‚è¿™ä¸ªèŒƒå›´æ˜¯ç»è¿‡å¤§é‡å®éªŒä¼˜åŒ–çš„ï¼š
- **å¤ªçŸ­ï¼ˆ<200 tokensï¼‰**ï¼šä¸Šä¸‹æ–‡ä¿¡æ¯ä¸è¶³ï¼Œéš¾ä»¥å‡†ç¡®è¯†åˆ«å¤æ‚å…³ç³»
- **å¤ªé•¿ï¼ˆ>600 tokensï¼‰**ï¼šLLMæ³¨æ„åŠ›åˆ†æ•£ï¼Œå¯èƒ½é—æ¼é‡è¦ä¿¡æ¯

```python
def chunk_text(document, max_tokens=400, overlap=50):
    """
    æ™ºèƒ½æ–‡æœ¬åˆ†å—ç­–ç•¥
    """
    # ä¼˜å…ˆæŒ‰è‡ªç„¶è¾¹ç•Œåˆ†å‰²ï¼ˆæ®µè½ã€å¥å­ï¼‰
    sentences = split_into_sentences(document)
    
    chunks = []
    current_chunk = ""
    current_tokens = 0
    
    for sentence in sentences:
        sentence_tokens = count_tokens(sentence)
        
        if current_tokens + sentence_tokens > max_tokens:
            # ä¿å­˜å½“å‰å—å¹¶å¼€å§‹æ–°å—
            if current_chunk:
                chunks.append(current_chunk)
            current_chunk = sentence
            current_tokens = sentence_tokens
        else:
            current_chunk += " " + sentence
            current_tokens += sentence_tokens
    
    # æ·»åŠ é‡å ä»¥ä¿æŒè¿ç»­æ€§
    return add_overlap(chunks, overlap)
```

> **ğŸ’¡ æç¤º**ï¼šé‡å ï¼ˆoverlapï¼‰æŠ€æœ¯èƒ½å¤Ÿé¿å…åœ¨åˆ†å—è¾¹ç•Œå¤„ä¸¢å¤±é‡è¦çš„å…³ç³»ä¿¡æ¯ï¼Œé€šå¸¸è®¾ç½®ä¸ºåˆ†å—å¤§å°çš„10-15%ã€‚

**2. å¹¶è¡ŒæŠ½å–**

ä½¿ç”¨LLMåŒæ—¶è¯†åˆ«å®ä½“ã€å…³ç³»ç±»å‹å’Œè¯¦ç»†æè¿°ã€‚ç°ä»£GraphRAGç³»ç»Ÿé‡‡ç”¨ç»“æ„åŒ–è¾“å‡ºï¼Œç¡®ä¿æŠ½å–ç»“æœçš„ä¸€è‡´æ€§ã€‚

```json
{
  "entities": [
    {
      "name": "è‹¹æœå…¬å¸",
      "type": "ORGANIZATION", 
      "description": "ç¾å›½è·¨å›½ç§‘æŠ€å…¬å¸ï¼Œä»¥è®¾è®¡å’Œåˆ¶é€ æ¶ˆè´¹ç”µå­äº§å“é—»å",
      "confidence": 0.95
    }
  ],
  "relationships": [
    {
      "source": "å²è’‚å¤«Â·ä¹”å¸ƒæ–¯",
      "target": "è‹¹æœå…¬å¸", 
      "type": "FOUNDER",
      "description": "å²è’‚å¤«Â·ä¹”å¸ƒæ–¯æ˜¯è‹¹æœå…¬å¸çš„è”åˆåˆ›å§‹äºº",
      "confidence": 0.98
    }
  ]
}
```

**3. è´¨é‡å¢å¼º**

ä¸ºæ¯ä¸ªå®ä½“ç”Ÿæˆä¸°å¯Œçš„æè¿°ä¿¡æ¯ï¼Œæé«˜åç»­æ£€ç´¢çš„å‡†ç¡®æ€§ã€‚è¿™åŒ…æ‹¬ï¼š
- **å®ä½“é‡è¦æ€§è¯„åˆ†**ï¼šåŸºäºåœ¨æ–‡æ¡£ä¸­çš„å‡ºç°é¢‘ç‡å’Œä¸Šä¸‹æ–‡é‡è¦æ€§
- **å®ä½“ç±»å‹åˆ†ç±»**ï¼šä½¿ç”¨é¢„å®šä¹‰çš„æœ¬ä½“æˆ–åŠ¨æ€ç”Ÿæˆç±»å‹
- **å®ä½“å±æ€§æå–**ï¼šè¯†åˆ«æ—¶é—´ã€åœ°ç‚¹ã€æ•°å€¼ç­‰ç»“æ„åŒ–å±æ€§

#### å…³é”®æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ

**æŒ‘æˆ˜1ï¼šé•¿è·ç¦»å…³ç³»è¯†åˆ«**

ä¼ ç»Ÿçš„æ»‘åŠ¨çª—å£æ–¹æ³•å¯èƒ½æ— æ³•æ•è·è·¨æ®µè½çš„å…³ç³»ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šå¤šæ–‡æœ¬å—ä¿¡æ¯èšåˆ
```python
def aggregate_cross_chunk_relations(chunks_relations):
    """
    èšåˆè·¨æ–‡æœ¬å—çš„å…³ç³»ä¿¡æ¯
    """
    entity_mentions = defaultdict(list)
    
    # æ”¶é›†æ‰€æœ‰å®ä½“æåŠ
    for chunk_id, relations in chunks_relations.items():
        for entity in relations['entities']:
            entity_mentions[entity['name']].append({
                'chunk_id': chunk_id,
                'context': entity['description'],
                'confidence': entity['confidence']
            })
    
    # åŸºäºå…±ç°å’Œè¯­ä¹‰ç›¸ä¼¼åº¦å»ºç«‹å…³ç³»
    cross_chunk_relations = []
    for entity_pair in itertools.combinations(entity_mentions.keys(), 2):
        relation = infer_relation_from_co_occurrence(
            entity_mentions[entity_pair[0]], 
            entity_mentions[entity_pair[1]]
        )
        if relation:
            cross_chunk_relations.append(relation)
    
    return cross_chunk_relations
```

**æŒ‘æˆ˜2ï¼šå®ä½“æ¶ˆæ­§**

åŒä¸€å®ä½“å¯èƒ½æœ‰å¤šç§è¡¨è¿°æ–¹å¼ï¼ˆå¦‚"è‹¹æœ"ã€"è‹¹æœå…¬å¸"ã€"Apple Inc."ï¼‰ã€‚

> **ğŸ“š çŸ¥è¯†ç‚¹æ³¨é‡Š**ï¼šå®ä½“æ¶ˆæ­§æ˜¯NLPä¸­çš„ç»å…¸é—®é¢˜ï¼Œç›®æ ‡æ˜¯ç¡®å®šæ–‡æœ¬ä¸­æåŠçš„å®ä½“æŒ‡å‘ç°å®ä¸–ç•Œä¸­çš„å“ªä¸ªå…·ä½“å¯¹è±¡ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šåˆ©ç”¨ç¤¾åŒºæ£€æµ‹çš„èšç±»ç‰¹æ€§å’Œè¯­ä¹‰ç›¸ä¼¼æ€§
```python
def entity_disambiguation(entities, embeddings_model):
    """
    åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦çš„å®ä½“æ¶ˆæ­§
    """
    # è®¡ç®—å®ä½“æè¿°çš„å‘é‡è¡¨ç¤º
    entity_embeddings = {}
    for entity in entities:
        embedding = embeddings_model.encode(
            f"{entity['name']} {entity['description']}"
        )
        entity_embeddings[entity['id']] = embedding
    
    # ä½¿ç”¨èšç±»ç®—æ³•è¯†åˆ«åŒä¸€å®ä½“çš„ä¸åŒè¡¨è¿°
    similarity_matrix = compute_cosine_similarity(entity_embeddings)
    clusters = hierarchical_clustering(similarity_matrix, threshold=0.8)
    
    # åˆå¹¶åŒä¸€èšç±»ä¸­çš„å®ä½“
    canonical_entities = merge_clustered_entities(entities, clusters)
    
    return canonical_entities
```

**æŒ‘æˆ˜3ï¼šå…³ç³»ä¸€è‡´æ€§**

ç›¸åŒè¯­ä¹‰çš„å…³ç³»å¯èƒ½è¢«è¡¨è¿°ä¸ºä¸åŒçš„å…³ç³»ç±»å‹ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šåå¤„ç†æ ‡å‡†åŒ–
```python
def normalize_relations(relations):
    """
    å…³ç³»ç±»å‹æ ‡å‡†åŒ–
    """
    # é¢„å®šä¹‰çš„å…³ç³»æ˜ å°„è§„åˆ™
    relation_mappings = {
        "åˆ›å»º": "FOUNDED",
        "å»ºç«‹": "FOUNDED", 
        "åˆ›ç«‹": "FOUNDED",
        "å·¥ä½œåœ¨": "WORKS_AT",
        "å°±èŒäº": "WORKS_AT",
        "æœåŠ¡äº": "WORKS_AT"
    }
    
    normalized_relations = []
    for relation in relations:
        # æ ‡å‡†åŒ–å…³ç³»ç±»å‹
        normalized_type = relation_mappings.get(
            relation['type'], 
            relation['type']
        )
        
        relation['type'] = normalized_type
        normalized_relations.append(relation)
    
    return normalized_relations
```

### å›¾æ„å»ºç­–ç•¥ï¼šä»éç»“æ„åŒ–åˆ°ç»“æ„åŒ–çš„æ™ºèƒ½è½¬æ¢

å›¾æ„å»ºæ˜¯GraphRAGçš„æ ¸å¿ƒç¯èŠ‚ï¼Œå†³å®šäº†æœ€ç»ˆç³»ç»Ÿçš„æ€§èƒ½ã€‚**å®ä½“ä¸ºä¸­å¿ƒçš„è®¾è®¡**ä½¿å¾—å›¾ç»“æ„ç›´è§‚ä¸”æ˜“äºæŸ¥è¯¢ï¼ŒåŒæ—¶æ”¯æŒåŠ¨æ€æ‰©å±•ã€‚

#### å…³é”®è®¾è®¡åŸåˆ™

**1. å±æ€§ä¸°å¯ŒåŒ–**

ä¸ºå®ä½“å’Œå…³ç³»æ·»åŠ è¯¦ç»†çš„å±æ€§ä¿¡æ¯ï¼ŒåŒ…æ‹¬æè¿°ã€é‡è¦æ€§è¯„åˆ†ã€ç½®ä¿¡åº¦ç­‰ã€‚

```python
class Entity:
    def __init__(self, name, entity_type, description):
        self.id = generate_unique_id(name, entity_type)
        self.name = name
        self.type = entity_type
        self.description = description
        self.importance_score = 0.0
        self.confidence = 0.0
        self.attributes = {}
        self.source_documents = []
        
    def add_attribute(self, key, value, confidence=1.0):
        """æ·»åŠ å®ä½“å±æ€§"""
        if key not in self.attributes:
            self.attributes[key] = []
        self.attributes[key].append({
            'value': value,
            'confidence': confidence,
            'timestamp': datetime.now()
        })

class Relationship:
    def __init__(self, source, target, rel_type, description):
        self.id = generate_unique_id(source, target, rel_type)
        self.source = source
        self.target = target  
        self.type = rel_type
        self.description = description
        self.weight = 1.0
        self.confidence = 0.0
        self.evidence = []
```

**2. å±‚æ¬¡åŒ–ç»„ç»‡**

é€šè¿‡Leidenç®—æ³•æ„å»ºå¤šå±‚æ¬¡çš„ç¤¾åŒºç»“æ„ï¼Œæ”¯æŒä¸åŒç²’åº¦çš„æŸ¥è¯¢ã€‚

> **ğŸ“š çŸ¥è¯†ç‚¹æ³¨é‡Š**ï¼šå±‚æ¬¡åŒ–ç¤¾åŒºæ£€æµ‹å…è®¸åœ¨ä¸åŒçš„ç²’åº¦çº§åˆ«ç†è§£ç½‘ç»œç»“æ„ï¼Œä»ç»†ç²’åº¦çš„å±€éƒ¨ç¤¾åŒºåˆ°ç²—ç²’åº¦çš„å…¨å±€æ¨¡å¼ã€‚

```python
def build_hierarchical_communities(graph):
    """
    æ„å»ºå¤šå±‚æ¬¡ç¤¾åŒºç»“æ„
    """
    levels = []
    current_graph = graph.copy()
    
    # é€å±‚è¿›è¡Œç¤¾åŒºæ£€æµ‹
    for level in range(max_levels):
        # è¿è¡ŒLeidenç®—æ³•
        communities = leiden_algorithm(current_graph)
        levels.append(communities)
        
        # å¦‚æœä¸èƒ½è¿›ä¸€æ­¥åˆ’åˆ†ï¼Œé€€å‡º
        if len(communities) == len(current_graph.nodes()):
            break
            
        # æ„å»ºä¸‹ä¸€å±‚çš„èšåˆå›¾
        current_graph = aggregate_communities_to_graph(
            current_graph, communities
        )
    
    return levels
```

**3. å¢é‡æ›´æ–°æ”¯æŒ**

ç»´æŠ¤å®ä½“IDçš„ä¸€è‡´æ€§ï¼Œæ”¯æŒçŸ¥è¯†å›¾è°±çš„åŠ¨æ€æ›´æ–°ã€‚

```python
class GraphBuilder:
    def __init__(self):
        self.entity_index = {}  # å®ä½“åç§°åˆ°IDçš„æ˜ å°„
        self.graph = nx.Graph()
        
    def add_or_update_entity(self, entity_data):
        """å¢é‡æ·»åŠ æˆ–æ›´æ–°å®ä½“"""
        entity_key = f"{entity_data['name']}_{entity_data['type']}"
        
        if entity_key in self.entity_index:
            # æ›´æ–°ç°æœ‰å®ä½“
            entity_id = self.entity_index[entity_key]
            self.merge_entity_data(entity_id, entity_data)
        else:
            # æ·»åŠ æ–°å®ä½“
            entity_id = self.create_new_entity(entity_data)
            self.entity_index[entity_key] = entity_id
            
        return entity_id
    
    def merge_entity_data(self, entity_id, new_data):
        """åˆå¹¶å®ä½“æ•°æ®"""
        current_data = self.graph.nodes[entity_id]
        
        # åˆå¹¶æè¿°ï¼ˆå–æœ€è¯¦ç»†çš„ï¼‰
        if len(new_data['description']) > len(current_data['description']):
            current_data['description'] = new_data['description']
            
        # æ›´æ–°ç½®ä¿¡åº¦ï¼ˆå–æœ€é«˜çš„ï¼‰
        current_data['confidence'] = max(
            current_data['confidence'], 
            new_data['confidence']
        )
        
        # åˆå¹¶å±æ€§
        for key, value in new_data.get('attributes', {}).items():
            if key not in current_data['attributes']:
                current_data['attributes'][key] = value
```

### å¤šè·³æ¨ç†æœºåˆ¶ï¼šå®ç°å¤æ‚æŸ¥è¯¢çš„å…³é”®

GraphRAGçš„å¤šè·³æ¨ç†èƒ½åŠ›æ˜¯å…¶æœ€å¤§äº®ç‚¹ã€‚ç³»ç»Ÿé€šè¿‡**å±€éƒ¨æœç´¢**å’Œ**å…¨å±€æœç´¢**ä¸¤ç§äº’è¡¥çš„æœºåˆ¶å¤„ç†ä¸åŒç±»å‹çš„æŸ¥è¯¢ã€‚

#### å±€éƒ¨æœç´¢ï¼šç²¾ç¡®çš„å®ä½“å…³ç³»éå†

å±€éƒ¨æœç´¢ä»æŸ¥è¯¢å®ä½“å¼€å§‹ï¼Œé€šè¿‡å›¾éå†æ‰©å±•åˆ°ç›¸å…³å®ä½“ï¼Œé€‚ç”¨äºå…·ä½“çš„äº‹å®æ€§æŸ¥è¯¢å’Œç»†èŠ‚é—®é¢˜ã€‚

```python
def local_search(graph, query_entities, max_hops=3, max_entities=50):
    """
    å±€éƒ¨å›¾æœç´¢å®ç°
    """
    # åˆå§‹åŒ–æœç´¢çŠ¶æ€
    visited = set()
    current_entities = set(query_entities)
    search_path = []
    
    for hop in range(max_hops):
        next_entities = set()
        hop_relations = []
        
        # æ‰©å±•å½“å‰å®ä½“çš„é‚»å±…
        for entity in current_entities:
            if entity in visited:
                continue
                
            neighbors = graph.neighbors(entity)
            for neighbor in neighbors:
                if len(next_entities) >= max_entities:
                    break
                    
                # è®°å½•å…³ç³»è·¯å¾„
                relation_data = graph.get_edge_data(entity, neighbor)
                hop_relations.append({
                    'source': entity,
                    'target': neighbor,
                    'relation': relation_data,
                    'hop': hop + 1
                })
                
                next_entities.add(neighbor)
        
        # æ›´æ–°æœç´¢çŠ¶æ€
        visited.update(current_entities)
        current_entities = next_entities
        search_path.append(hop_relations)
        
        # å¦‚æœæ²¡æœ‰æ–°çš„å®ä½“ï¼Œæå‰ç»ˆæ­¢
        if not next_entities:
            break
    
    return search_path, visited
```

**æœç´¢ä¼˜åŒ–ç­–ç•¥**ï¼š

1. **é‡è¦æ€§å¼•å¯¼æœç´¢**ï¼šä¼˜å…ˆæ¢ç´¢é‡è¦æ€§è¯„åˆ†é«˜çš„å®ä½“
2. **å…³ç³»ç±»å‹è¿‡æ»¤**ï¼šæ ¹æ®æŸ¥è¯¢æ„å›¾è¿‡æ»¤ç›¸å…³çš„å…³ç³»ç±»å‹
3. **è·¯å¾„å‰ªæ**ï¼šç§»é™¤ä½ç½®ä¿¡åº¦çš„æœç´¢è·¯å¾„

> **âš ï¸ æ³¨æ„**ï¼šå±€éƒ¨æœç´¢çš„æ·±åº¦éœ€è¦è°¨æ…è®¾ç½®ã€‚æ·±åº¦è¿‡æµ…å¯èƒ½é—æ¼é‡è¦ä¿¡æ¯ï¼Œæ·±åº¦è¿‡æ·±åˆ™ä¼šå¯¼è‡´è®¡ç®—å¤æ‚åº¦æŒ‡æ•°å¢é•¿ã€‚

#### å…¨å±€æœç´¢ï¼šåŸºäºç¤¾åŒºçš„Map-Reduceæ¨ç†

å…¨å±€æœç´¢åˆ©ç”¨åˆ†å±‚ç¤¾åŒºç»“æ„è¿›è¡Œå…¨å±€æ¨ç†ï¼Œé€‚ç”¨äºéœ€è¦æ•´ä½“ç†è§£çš„é—®é¢˜ã€‚

```python
def global_search(graph, communities, query, llm):
    """
    å…¨å±€æœç´¢çš„Map-Reduceå®ç°
    """
    # Mapé˜¶æ®µï¼šå¹¶è¡Œå¤„ç†å„ä¸ªç¤¾åŒº
    community_summaries = []
    
    with ThreadPoolExecutor(max_workers=4) as executor:
        future_to_community = {}
        
        for community_id, community_entities in communities.items():
            # ä¸ºæ¯ä¸ªç¤¾åŒºç”Ÿæˆæ‘˜è¦
            future = executor.submit(
                generate_community_summary,
                graph.subgraph(community_entities),
                query,
                llm
            )
            future_to_community[future] = community_id
        
        # æ”¶é›†ç¤¾åŒºæ‘˜è¦
        for future in as_completed(future_to_community):
            community_id = future_to_community[future]
            try:
                summary = future.result()
                community_summaries.append({
                    'community_id': community_id,
                    'summary': summary,
                    'entities_count': len(communities[community_id])
                })
            except Exception as exc:
                print(f'Community {community_id} generated an exception: {exc}')
    
    # Reduceé˜¶æ®µï¼šåˆå¹¶ç¤¾åŒºæ‘˜è¦
    global_summary = aggregate_community_summaries(
        community_summaries, query, llm
    )
    
    return global_summary

def generate_community_summary(subgraph, query, llm):
    """
    ç”Ÿæˆå•ä¸ªç¤¾åŒºçš„æ‘˜è¦
    """
    # æå–ç¤¾åŒºçš„å…³é”®ä¿¡æ¯
    entities = [data for node, data in subgraph.nodes(data=True)]
    relationships = [data for u, v, data in subgraph.edges(data=True)]
    
    # æ„å»ºprompt
    prompt = f"""
    åŸºäºä»¥ä¸‹å®ä½“å’Œå…³ç³»ä¿¡æ¯ï¼Œé’ˆå¯¹æŸ¥è¯¢"{query}"ç”Ÿæˆä¸€ä¸ªç®€æ´çš„æ‘˜è¦ï¼š
    
    å®ä½“ï¼š{format_entities(entities)}
    å…³ç³»ï¼š{format_relationships(relationships)}
    
    è¯·é‡ç‚¹å…³æ³¨ä¸æŸ¥è¯¢ç›¸å…³çš„ä¿¡æ¯ï¼Œç”Ÿæˆä¸è¶…è¿‡200å­—çš„æ‘˜è¦ã€‚
    """
    
    summary = llm.generate(prompt)
    return summary
```

### æ··åˆæ£€ç´¢ç­–ç•¥ï¼šç»“åˆç»“æ„åŒ–ä¸éç»“æ„åŒ–æ£€ç´¢

GraphRAGçš„çœŸæ­£å¨åŠ›åœ¨äºå…¶æ··åˆæ£€ç´¢ç­–ç•¥ï¼ŒåŒæ—¶åˆ©ç”¨**å‘é‡ç›¸ä¼¼åº¦**å’Œ**å›¾ç»“æ„å…³ç³»**è¿›è¡Œæ£€ç´¢ã€‚

#### æ£€ç´¢æ¶æ„è®¾è®¡

```python
class HybridRetriever:
    def __init__(self, graph, vector_index, embeddings_model):
        self.graph = graph
        self.vector_index = vector_index
        self.embeddings_model = embeddings_model
        
    def retrieve(self, query, method='auto', top_k=10):
        """
        æ··åˆæ£€ç´¢å…¥å£
        """
        if method == 'auto':
            method = self.determine_retrieval_method(query)
            
        if method == 'vector':
            return self.vector_retrieval(query, top_k)
        elif method == 'graph':
            return self.graph_retrieval(query, top_k)
        else:  # hybrid
            return self.hybrid_retrieval(query, top_k)
    
    def determine_retrieval_method(self, query):
        """
        æ ¹æ®æŸ¥è¯¢ç‰¹å¾è‡ªåŠ¨é€‰æ‹©æ£€ç´¢æ–¹æ³•
        """
        # æ£€æµ‹æŸ¥è¯¢ç±»å‹
        if self.is_factual_query(query):
            return 'vector'
        elif self.is_multi_hop_query(query):
            return 'graph'
        else:
            return 'hybrid'
```

#### æ™ºèƒ½è·¯ç”±ç­–ç•¥

åŸºäºæŸ¥è¯¢ç‰¹å¾è‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„æ£€ç´¢ç­–ç•¥ï¼š

```python
def classify_query_type(query):
    """
    æŸ¥è¯¢ç±»å‹åˆ†ç±»
    """
    # å…³é”®è¯æ¨¡å¼åŒ¹é…
    multi_hop_patterns = [
        r'.*çš„.*çš„.*',  # "Açš„Bçš„C"æ¨¡å¼
        r'å¦‚ä½•.*å½±å“.*',  # å› æœå…³ç³»æŸ¥è¯¢
        r'.*ä¹‹é—´.*å…³ç³»',  # å…³ç³»æŸ¥è¯¢
        r'.*å¯¼è‡´.*'      # å› æœé“¾æŸ¥è¯¢
    ]
    
    factual_patterns = [
        r'.*æ˜¯ä»€ä¹ˆ',
        r'.*çš„å®šä¹‰', 
        r'ä»€ä¹ˆæ˜¯.*',
        r'.*çš„æ¦‚å¿µ'
    ]
    
    global_patterns = [
        r'ä¸»è¦.*ä¸»é¢˜',
        r'æ•´ä½“.*æƒ…å†µ',
        r'æ€»ä½“.*è¶‹åŠ¿',
        r'å…¨å±€.*åˆ†æ'
    ]
    
    # æ¨¡å¼åŒ¹é…
    for pattern in multi_hop_patterns:
        if re.match(pattern, query):
            return 'multi_hop'
    
    for pattern in factual_patterns:
        if re.match(pattern, query):
            return 'factual'
            
    for pattern in global_patterns:
        if re.match(pattern, query):
            return 'global'
    
    return 'hybrid'  # é»˜è®¤ä½¿ç”¨æ··åˆæ£€ç´¢
```

#### ç»“æœèåˆæŠ€æœ¯

å°†æ¥è‡ªä¸åŒæ£€ç´¢è·¯å¾„çš„ç»“æœè¿›è¡Œæ™ºèƒ½èåˆï¼š

```python
def fuse_retrieval_results(graph_results, vector_results, weights=[0.6, 0.4]):
    """
    èåˆå›¾æ£€ç´¢å’Œå‘é‡æ£€ç´¢çš„ç»“æœ
    """
    # æ ‡å‡†åŒ–è¯„åˆ†
    graph_scores = normalize_scores([r['score'] for r in graph_results])
    vector_scores = normalize_scores([r['score'] for r in vector_results])
    
    # åŸºäºå®ä½“é‡å åº¦è¿›è¡Œèåˆ
    fused_results = []
    
    for i, graph_result in enumerate(graph_results):
        graph_entities = set(graph_result['entities'])
        best_vector_match = None
        best_overlap = 0
        
        for j, vector_result in enumerate(vector_results):
            vector_entities = set(vector_result['entities'])
            overlap = len(graph_entities & vector_entities)
            
            if overlap > best_overlap:
                best_overlap = overlap
                best_vector_match = (j, vector_result, vector_scores[j])
        
        # è®¡ç®—èåˆåˆ†æ•°
        if best_vector_match:
            j, vector_result, vector_score = best_vector_match
            fused_score = (
                weights[0] * graph_scores[i] + 
                weights[1] * vector_score
            )
            
            # åˆå¹¶ç»“æœ
            fused_result = {
                'content': merge_content(graph_result, vector_result),
                'score': fused_score,
                'sources': graph_result['sources'] + vector_result['sources'],
                'reasoning_path': graph_result.get('reasoning_path', [])
            }
            
            fused_results.append(fused_result)
    
    return sorted(fused_results, key=lambda x: x['score'], reverse=True)
```

## æ€§èƒ½ä¼˜åŒ–ä¸æœ€ä½³å®è·µ

### ç´¢å¼•ä¼˜åŒ–

1. **åˆ†å±‚ç´¢å¼•**ï¼šæ„å»ºå¤šå±‚æ¬¡çš„ç´¢å¼•ç»“æ„
2. **ç¼“å­˜ç­–ç•¥**ï¼šç¼“å­˜é¢‘ç¹æŸ¥è¯¢çš„ç»“æœ
3. **å¹¶è¡Œå¤„ç†**ï¼šåˆ©ç”¨å¤šçº¿ç¨‹å’Œå¼‚æ­¥å¤„ç†æé«˜æ€§èƒ½

### å†…å­˜ç®¡ç†

```python
def optimize_graph_memory(graph, threshold=0.1):
    """
    å›¾å†…å­˜ä¼˜åŒ–
    """
    # ç§»é™¤ä½é‡è¦æ€§çš„å®ä½“å’Œå…³ç³»
    low_importance_entities = [
        node for node, data in graph.nodes(data=True)
        if data.get('importance_score', 0) < threshold
    ]
    
    graph.remove_nodes_from(low_importance_entities)
    
    # å‹ç¼©å®ä½“æè¿°
    for node, data in graph.nodes(data=True):
        if 'description' in data and len(data['description']) > 500:
            data['description'] = data['description'][:500] + '...'
    
    return graph
```

## å°ç»“

æœ¬æ–‡æ·±å…¥åˆ†æäº†GraphRAGçš„æ ¸å¿ƒæŠ€æœ¯å®ç°ï¼ŒåŒ…æ‹¬å®ä½“å…³ç³»æŠ½å–ã€å›¾æ„å»ºç­–ç•¥ã€å¤šè·³æ¨ç†æœºåˆ¶å’Œæ··åˆæ£€ç´¢æŠ€æœ¯ã€‚è¿™äº›æŠ€æœ¯çš„æœ‰æœºç»“åˆä½¿å¾—GraphRAGèƒ½å¤Ÿå¤„ç†ä¼ ç»ŸRAGæ— æ³•åº”å¯¹çš„å¤æ‚æŸ¥è¯¢ã€‚

**æœ¬æ–‡è¦ç‚¹å›é¡¾**ï¼š
- âœ… LLMé©±åŠ¨çš„å®ä½“å…³ç³»æŠ½å–æä¾›é«˜è´¨é‡çš„çŸ¥è¯†æå–
- âœ… å±‚æ¬¡åŒ–å›¾æ„å»ºæ”¯æŒå¤šç²’åº¦çš„çŸ¥è¯†ç»„ç»‡
- âœ… å±€éƒ¨å’Œå…¨å±€æœç´¢äº’è¡¥ï¼Œè¦†ç›–ä¸åŒç±»å‹çš„æŸ¥è¯¢éœ€æ±‚
- âœ… æ··åˆæ£€ç´¢ç­–ç•¥ç»“åˆç»“æ„åŒ–å’Œéç»“æ„åŒ–æ£€ç´¢çš„ä¼˜åŠ¿

**ä¸‹æœŸé¢„å‘Š**ï¼šã€ŠGraphRAG å®Œæ•´å…¥é—¨æŒ‡å—ï¼ˆä¸‰ï¼‰ï¼šå·¥å…·æ¡†æ¶æ·±åº¦åˆ†æã€‹å°†è¯¦ç»†ä»‹ç»Neo4jã€LangChainå’ŒMicrosoft GraphRAGç­‰ä¸»æµå·¥å…·æ¡†æ¶çš„ç‰¹ç‚¹å’Œä½¿ç”¨æ–¹æ³•ã€‚ 